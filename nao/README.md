# Q-Learning para Controle de Articula√ß√£o do Rob√¥ NAO

Este projeto utiliza o algoritmo de **Q-Learning** para treinar o rob√¥ NAO a ajustar o √¢ngulo de uma articula√ß√£o espec√≠fica (por padr√£o, o `LShoulderPitch`) at√© atingir um valor alvo. O treinamento considera intera√ß√µes reais com o rob√¥, aplicando a√ß√µes discretas e avaliando a recompensa com base na proximidade do √¢ngulo atingido em rela√ß√£o ao alvo.

## üöÄ Tecnologias Utilizadas

* Python 2.7
* Naoqi SDK
* NumPy
* Rob√¥ NAO (f√≠sico ou simulado)
* Comunica√ß√£o via IP

## üß† Descri√ß√£o do Algoritmo

O agente utiliza o **Q-Learning**, uma t√©cnica de aprendizado por refor√ßo que busca aprender uma pol√≠tica √≥tima atrav√©s da explora√ß√£o e atualiza√ß√£o de uma tabela Q.
Durante o processo:

1. A articula√ß√£o come√ßa em um √¢ngulo atual.
2. O agente escolhe uma a√ß√£o (movimento) com base em Œµ-greedy.
3. A a√ß√£o √© aplicada no rob√¥, movendo a articula√ß√£o.
4. A nova posi√ß√£o √© avaliada e uma recompensa √© atribu√≠da.
5. A tabela Q √© atualizada com base na recompensa e no melhor valor futuro estimado.

## ‚öôÔ∏è Par√¢metros

* `alpha`: Taxa de aprendizado (0.2)
* `gamma`: Fator de desconto (0.9)
* `epsilon`: Probabilidade de explora√ß√£o (0.2)
* `n_actions`: N√∫mero de a√ß√µes poss√≠veis (5)
* `n_states`: Discretiza√ß√£o do espa√ßo de estados (100)
* `ANGLE_LIMIT`: Limite de movimento da articula√ß√£o em radianos (¬±90¬∞)
* `EPISODES`: N√∫mero de epis√≥dios de treinamento (100)
* `MAX_STEPS`: Passos por epis√≥dio (1)

## üìÅ Arquivos

* `main.py`: C√≥digo principal com a l√≥gica de conex√£o, execu√ß√£o e aprendizado.
* `qlearning_log_NAO.csv`: Log de cada epis√≥dio (recompensa, posi√ß√£o, etc.).
* `q_table.txt`: Arquivo onde a tabela Q pode ser salva (atualmente desabilitado).

## üìù Como Usar

1. Conecte o rob√¥ NAO na mesma rede do seu computador.

2. Instale o SDK `naoqi` compat√≠vel com Python 2.7.

3. Edite a linha com o IP do seu rob√¥:

   ```python
   robot_ip = "172.15.0.136"
   ```

4. Execute o script:

   ```bash
   python main.py
   ```

5. Acompanhe o log em `qlearning_log_NAO.csv`.

## üìä Sa√≠da Esperada

Durante o treinamento, o terminal mostrar√°:

* Epis√≥dio atual
* Recompensa total
* Posi√ß√£o normalizada e em radianos
* √Çngulo em graus
* Recompensa daquele passo
* Alerta se o alvo foi atingido

## ‚úÖ Crit√©rio de Parada

O treinamento termina automaticamente se o √¢ngulo da articula√ß√£o atingir o valor alvo com uma toler√¢ncia de `¬±0.05 rad`.

## üìå Observa√ß√µes

* Ajuste o nome da articula√ß√£o (`joint_name`) caso queira treinar outra parte do rob√¥.
* Os limites de √¢ngulo podem ser refinados conforme os limites f√≠sicos do NAO.
* O c√≥digo foi testado com a articula√ß√£o `LShoulderPitch`.

